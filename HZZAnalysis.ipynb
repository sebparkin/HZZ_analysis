{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to rediscover the Higgs boson yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<CENTER><img src=\"../../images/ATLASOD.gif\" style=\"width:50%\"></CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses ATLAS Open Data https://opendata.atlas.cern to show you the steps to rediscover the Higgs boson yourself!\n",
    "\n",
    "ATLAS Open Data provides open access to proton-proton collision data at the LHC for educational purposes. ATLAS Open Data resources are ideal for high-school, undergraduate and postgraduate students.\n",
    "\n",
    "Notebooks are web applications that allow you to create and share documents that can contain for example:\n",
    "1. live code\n",
    "2. visualisations\n",
    "3. narrative text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Higgs boson? \n",
    "The Higgs boson is a fundamental particle predicted by the Standard Model. \n",
    "It is a manifestation of the Higgs field,\n",
    "    which gives mass to the fundamental particles.\n",
    "However,\n",
    "    it is incredibly hard to produce.\n",
    "At the LHC, \n",
    "    a Higgs particle is produced about once every 10 billion collisions!\n",
    "This tiny fraction makes it very difficult to detect.\n",
    "Nevertheless, \n",
    "    after years of data collection, \n",
    "    the Higgs boson was finally discovered in 2012 by CMS and ATLAS experiments at CERN.\n",
    "In this tutorial, \n",
    "    we shall be following their example. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting the Higgs\n",
    "This analysis loosely follows the paper on the [discovery of the Higgs boson by ATLAS](https://www.sciencedirect.com/science/article/pii/S037026931200857X) (mostly Section 4 and 4.1).\n",
    "\n",
    "The Higgs boson can be produced in many different ways. \n",
    "In particle physics, \n",
    "    we describe these production modes using Feynman diagrams.\n",
    "These diagrams allow us to visualise particle processes while also acting as powerful tools for calculations.\n",
    "See [here](https://cds.cern.ch/record/2759490/files/Feynman%20Diagrams%20-%20ATLAS%20Cheat%20Sheet.pdf) for more information on Feynman diagrams.\n",
    "\n",
    "There are four main production modes of the Higgs boson, and their respective Feynman diagrams:\n",
    "1. Gluon-gluon fusion (top left)\n",
    "2. Vector boson fusion (top right)\n",
    "3. Vector boson bremsstrahlung (bottom left)\n",
    "4. Top-antitop fusion (bottom right)\n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/ggH.png\" style=\"width:40%\"> <img src=\"images/ImagesHiggs/VBFH.png\" style=\"width:35%\"></CENTER>\n",
    "<CENTER><img src=\"images/ImagesHiggs/WH.png\" style=\"width:40%\"> <img src=\"images/ImagesHiggs/ttbarfusion.png\" style=\"width:35%\"></CENTER>\n",
    "\n",
    "The Higgs has a very short lifetime,\n",
    "    on the order of $10^{-22} \\,\\text{s}$.\n",
    "It decays extremely quickly after production,\n",
    "    so there is no hope of directly detecting the particle.\n",
    "Nevertheless,\n",
    "    we can use the Standard Model to predict its \n",
    "    decay products: photons, Z bosons, quarks, etc.,\n",
    "    all with different probabilities.\n",
    "These **decay channels** can be used to identify the Higgs boson.\n",
    "In this notebook, \n",
    "    we'll be looking at one particular decay channel:\n",
    "$$H \\rightarrow ZZ^* \\rightarrow \\ell\\ell\\ell\\ell$$\n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/HZZ_feynman.png\" style=\"width:40%\"></CENTER>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to this as our desired **signal**.\n",
    "Ideally,\n",
    "    we would search for collisions which yield four leptons as products and this would tell us that a Higgs boson is present.\n",
    "Unfortunately,\n",
    "    in addition to our signal,\n",
    "    there are many other **background** processes that lead to four reconstructed leptons in the final state. \n",
    "The main background is $ZZ^*  \\to \\ell\\ell\\ell\\ell$,\n",
    "    where decay products have the same properties as those in the Higgs decay. \n",
    "This is known as an irreducible background. \n",
    "<CENTER><img src=\"images/ImagesHiggs/ZZllll.png\" style=\"width:40%\"></CENTER>\n",
    "\n",
    "We can get around this by accounting for the total invariant mass of the lepton products. \n",
    "We know through conservation of energy and momentum that the invariant mass of the products must be equal to the Higgs mass, \n",
    "    while other background processes will have different invariant masses. \n",
    "Our last step would be to plot the invariant mass of each event and spot the peak in mass around $125\\, \\text{GeV}$ , which corresponds to the mass of the Higgs boson. \n",
    "\n",
    "We also have background contributions from $Z +$ jets and top-anti top processes, \n",
    "    where additional charged leptons can arise either from semi-leptonic decays of heavy flavour or light flavour jets misidentified as leptons.\n",
    "These backgrounds are difficult to remove completely. \n",
    "\n",
    "<CENTER><img src=\"images/ImagesHiggs/Zllll.png\" style=\"width:30%\"><img src=\"images/ImagesHiggs/ttbar.png\" style=\"width:30%\"></CENTER>\n",
    "\n",
    "For such processes,\n",
    "    we will attempt to distinguish them from the Higgs decay using the properties of the leptons.\n",
    "Because the Higgs is a neutral particle with zero lepton number,\n",
    "    the lepton products from its decay must sum to zero charge and zero lepton numbers.\n",
    "Thus, \n",
    "    we can cut away all data with products that do not have these properties.\n",
    "These cuts increase the ratio of our signal to the reducible background.\n",
    "\n",
    "Note: $Z^*$ refers to a $Z$ boson that is off its mass shell. \n",
    "This means that its mass is not fixed to the $91 \\, \\text{GeV}$ of a typical $Z$ boson. \n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "1. Learn to process large data sets using cuts\n",
    "2. Understand some general principles of a particle physics analysis\n",
    "3. Discover the Higgs boson!\n",
    "\n",
    "See [here](https://cds.cern.ch/record/2800577/files/Signal%20and%20Background%20Physics%20Cheat%20Sheet.pdf) for more information on signals and backgrounds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Python notebook\n",
    "A Python notebook consists of cell blocks, \n",
    "    each containing lines of Python code.\n",
    "Each cell can be run independently of each other,\n",
    "    yielding respective outputs below the cells.\n",
    "Conventionally,\n",
    "    cells are run in order from top to bottom.\n",
    "\n",
    "\n",
    "- To run the whole notebook, in the top menu click Cell $\\to$ Run All.\n",
    "\n",
    "- To propagate a change you've made to a piece of code, click Cell $\\to$ Run All Below.\n",
    "\n",
    "- You can also run a single code cell, by clicking Cell $\\to$ Run Cells, or using the keyboard shortcut Shift+Enter.\n",
    "\n",
    "For more information, \n",
    "    refer to [here](https://www.codecademy.com/article/how-to-use-jupyter-notebooks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATLAS Open Data Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First time package installation on your computer (not needed on mybinder)\n",
    "This first cell installs the required python packages.\n",
    "It only needs to be run the first time you open this notebook on your computer. \n",
    "If you close Jupyter and re-open on the same computer, you won't need to run this first cell again.\n",
    "\n",
    "If this is opened on mybinder, you don't need to run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# # update the pip package installer\n",
    "!{sys.executable} -m pip install --upgrade --user pip\n",
    "# # install required packages\n",
    "!{sys.executable} -m pip install --upgrade --user uproot awkward vector numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to import a number of packages to help us:\n",
    "* `numpy`: provides numerical calculations such as histogramming\n",
    "* `matplotlib`: common tool for making plots, figures, images, visualisations\n",
    "* `uproot`: processes `.root` files typically used in particle physics into data formats used in python\n",
    "* `awkward`: introduces `awkward` arrays, a format that generalizes `numpy` to nested data with possibly variable length lists\n",
    "* `vector`: to allow vectorized 4-momentum calculations\n",
    "\n",
    "We also import the file `infofile`, which contains all relevant information of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import infofile # local file containing cross-sections, sums of weights, dataset IDs\n",
    "import numpy as np # for numerical calculations such as histogramming\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib_inline # to edit the inline plot format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg') # to make plots in pdf (vector) format\n",
    "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
    "import uproot # for reading .root files\n",
    "import awkward as ak # to represent nested data in columnar format\n",
    "import vector # for 4-momentum calculations\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit definitions, as stored in the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeV = 0.001\n",
    "GeV = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Reading data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to read some of the data from the open dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATLAS Open Data directory\n",
    "path = \"https://atlas-opendata.web.cern.ch/atlas-opendata/samples/2020/4lep/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenient naming and identification purposes,\n",
    "    we define a dictionary which stores all the important names of the samples we want to pull from the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For identification and naming\n",
    "samples = {\n",
    "\n",
    "    'data': {\n",
    "        'list' : ['data_A','data_B','data_C','data_D'], # data is from 2016, first four periods of data taking (ABCD)\n",
    "    },\n",
    "\n",
    "    r'Background $Z,t\\bar{t}$' : { # Z + ttbar\n",
    "        'list' : ['Zee','Zmumu','ttbar_lep'],\n",
    "        'color' : \"#6b59d3\" # purple\n",
    "    },\n",
    "\n",
    "    r'Background $ZZ^*$' : { # ZZ\n",
    "        'list' : ['llll'],\n",
    "        'color' : \"#ff0000\" # red\n",
    "    },\n",
    "\n",
    "    r'Signal ($m_H$ = 125 GeV)' : { # H -> ZZ -> llll\n",
    "        'list' : ['ggH125_ZZ4lep','VBFH125_ZZ4lep','WH125_ZZ4lep','ZH125_ZZ4lep'],\n",
    "        'color' : \"#00cdff\" # light blue\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key named `data` refers to the event information collected from real experiments,\n",
    "    while the `Background` and `Signal` keys refer to Monte-Carlo (MC) simulations of the ATLAS experiments.\n",
    "Both real data and MC data will then be analysed and compared together to discover the Higgs! \n",
    "\n",
    "Let's try accessing `data_A` in the Open Data URL as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': ['data_A', 'data_B', 'data_C', 'data_D']}\n",
      "value='data_A'\n"
     ]
    }
   ],
   "source": [
    "# Print the samples dict for the key 'data'\n",
    "print(samples['data'])\n",
    "\n",
    "# We shall use the first entry in 'list', 'data_A'\n",
    "value = samples['data']['list'][0]\n",
    "print(f\"{value=}\")\n",
    "\n",
    "# This is now appended to our file path to retrieve the data_A.4lep.root file\n",
    "data_A_path = path + \"Data/\" + value + \".4lep.root\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we shall try opening the `data_A` file to see what is inside.\n",
    "In the file (called a `tree`),\n",
    "    there are 39 entries, \n",
    "    one for each event.\n",
    "In each event,\n",
    "    a dictionary stores the all relevant information as keys, such as the event number (`eventNumber`), lepton transverse momentum (`lep_pt`), etc.  \n",
    "Details on the variables in the dictionary can be viewed [here](https://cds.cern.ch/record/2707171/files/ANA-OTRC-2019-01-PUB-updated.pdf) in Appendix A.\n",
    "\n",
    "More information on trees can be viewed [here](https://masonproffitt.github.io/uproot-tutorial/03-trees/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['runNumber', 'eventNumber', 'channelNumber', 'mcWeight', 'scaleFactor_PILEUP', 'scaleFactor_ELE', 'scaleFactor_MUON', 'scaleFactor_PHOTON', 'scaleFactor_TAU', 'scaleFactor_BTAG', 'scaleFactor_LepTRIGGER', 'scaleFactor_PhotonTRIGGER', 'trigE', 'trigM', 'trigP', 'lep_n', 'lep_truthMatched', 'lep_trigMatched', 'lep_pt', 'lep_eta', 'lep_phi', 'lep_E', 'lep_z0', 'lep_charge', 'lep_type', 'lep_isTightID', 'lep_ptcone30', 'lep_etcone20', 'lep_trackd0pvunbiased', 'lep_tracksigd0pvunbiased', 'met_et', 'met_phi', 'jet_n', 'jet_pt', 'jet_eta', 'jet_phi', 'jet_E', 'jet_jvt', 'jet_trueflav', 'jet_truthMatched', 'jet_MV2c10', 'photon_n', 'photon_truthMatched', 'photon_trigMatched', 'photon_pt', 'photon_eta', 'photon_phi', 'photon_E', 'photon_isTightID', 'photon_ptcone30', 'photon_etcone20', 'photon_convType', 'tau_n', 'tau_pt', 'tau_eta', 'tau_phi', 'tau_E', 'tau_isTightID', 'tau_truthMatched', 'tau_trigMatched', 'tau_nTracks', 'tau_BDTid', 'ditau_m', 'lep_pt_syst', 'met_et_syst', 'jet_pt_syst', 'photon_pt_syst', 'tau_pt_syst', 'XSection', 'SumWeights', 'largeRjet_n', 'largeRjet_pt', 'largeRjet_eta', 'largeRjet_phi', 'largeRjet_E', 'largeRjet_m', 'largeRjet_truthMatched', 'largeRjet_D2', 'largeRjet_tau32', 'largeRjet_pt_syst', 'tau_charge']\n",
      "[{runNumber: 298773, eventNumber: 134827213, channelNumber: 298773, ...}, ...]\n"
     ]
    }
   ],
   "source": [
    "# Accessing the file from the online directory (\":mini\" opens the tree in a desired manner)\n",
    "tree = uproot.open(data_A_path + \":mini\")\n",
    "\n",
    "\n",
    "# There are 39 entries in the tree\n",
    "print(tree.num_entries)\n",
    "\n",
    "# We can view all the information stored in the tree using the .keys() method.\n",
    "print(tree.keys())\n",
    "\n",
    "# We can also view the entire tree using the .arrays() method\n",
    "# This generates a 39-entry list of dictionaries\n",
    "print(tree.arrays()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we'd like to see the lepton energies. \n",
    "We can access this from our tree using the key `lep_E`. \n",
    "Also, \n",
    "    from this point on we shall be manipulating our tree arrays using the `awkward` library.\n",
    "We can use `library=\"ak\"` in the argument of the `.arrays()` method to use this library.\n",
    "If you ever see `library=\"ak\"` in the code,\n",
    "    it means that the array is output as an `awkward` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>[{lep_E: [1.34e+05, 6.76e+04, 4.3e+04, 2.74e+04]},\n",
       " {lep_E: [7.11e+05, 2.09e+05, 1.18e+05, 4.43e+05]},\n",
       " {lep_E: [6.56e+04, 4.08e+04, 1.9e+04, 1.49e+04]},\n",
       " {lep_E: [1.85e+05, 2.04e+04, 1.63e+04, 1.36e+04]},\n",
       " {lep_E: [1.5e+05, 1.32e+05, 3.51e+04, 3.32e+04]},\n",
       " {lep_E: [3.17e+05, 1.07e+05, 1.17e+05, 4.98e+04]},\n",
       " {lep_E: [1.4e+05, 3.08e+04, 2.2e+04, 2.07e+04]},\n",
       " {lep_E: [1.63e+05, 4.29e+04, 1.59e+04, 1.23e+04]},\n",
       " {lep_E: [1.42e+05, 3.12e+05, 1.57e+05, 9.7e+04]},\n",
       " {lep_E: [9.35e+04, 2.26e+04, 1.34e+04, 1.55e+04]},\n",
       " ...,\n",
       " {lep_E: [7.11e+04, 2.43e+04, 1.77e+04, 7.97e+03]},\n",
       " {lep_E: [1.23e+05, 3.04e+04, 8.38e+03, 1.69e+04]},\n",
       " {lep_E: [1.7e+05, 2.18e+05, 5.73e+04, 7.17e+03]},\n",
       " {lep_E: [1.5e+05, 7.31e+04, 2.71e+04, 1.97e+04]},\n",
       " {lep_E: [7.24e+04, 5.55e+04, 1.95e+04, 1.23e+04]},\n",
       " {lep_E: [7.82e+04, 7.52e+04, 1.69e+04, 1.68e+04]},\n",
       " {lep_E: [3.65e+04, 2.79e+04, 1.94e+04, 1.15e+04]},\n",
       " {lep_E: [1.37e+05, 5.14e+04, 1.7e+04, 2.5e+04]},\n",
       " {lep_E: [9.17e+04, 1.67e+04, 1.19e+04, 1.29e+04]}]\n",
       "---------------------------------------------------\n",
       "backend: cpu\n",
       "nbytes: 944 B\n",
       "type: 39 * {\n",
       "    lep_E: var * float32\n",
       "}</pre>"
      ],
      "text/plain": [
       "<Array [{lep_E: [1.34e+05, ...]}, ..., {...}] type='39 * {lep_E: var * floa...'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[\"lep_E\"].arrays(library=\"ak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our analysis, \n",
    "    not all the information in the tree is important.\n",
    "We can store the important variables in a list and retrieve them from the tree later on.\n",
    "As it turns out, \n",
    "    we will need the following set of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what variables are important to our analysis\n",
    "variables = ['lep_pt','lep_eta','lep_phi','lep_E','lep_charge','lep_type']\n",
    "\n",
    "# To see all the data for our given variables\n",
    "# for data in tree.iterate(variables, library=\"ak\"):\n",
    "#     print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to access the information in the `data_A` tree,\n",
    "    we can begin analysis.\n",
    "As mentioned in the introduction,\n",
    "    there are two key steps to be completed for each event entry:\n",
    "1. **Cuts** - we need to account for lepton selection rules in the event. \n",
    "In the [paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X), \n",
    "    it is stated that we must\n",
    "\"[select] two pairs of isolated leptons, each of which is comprised of two leptons with the **same flavour** and **opposite charge**\".\n",
    "The datasets used in this notebook have already been filtered to include at least 4 leptons per event.\n",
    "We need to filter the data such that in each event, there are pairs of leptons of the **same lepton type** (`lep_type`) and summing to **zero lepton charge** (`lep_charge`).\n",
    "\n",
    "2. **Mass calculation** - the data to be plotted is the 4-lepton invariant mass, which can be found using the equation: $$m_\\text{4l} = \\sqrt{E^2_\\text{tot}-\\mathbf{p}_\\text{tot}\\cdot\\mathbf{p}_\\text{tot}}$$\n",
    "in units where $c=1$.\n",
    "$E_\\text{tot}$ is the total energy and $\\mathbf{p}_\\text{tot}$ is the total momentum.\n",
    "This calculation is performed using the vector array method `.M` on the sum of lepton 4-momenta (`lep_pt`,`lep_eta`,`lep_phi`,`lep_E`).\n",
    "\n",
    "From this,\n",
    "    we can see why we chose those six important variables earlier. \n",
    "The physical reasoning for why we perform these steps is encapsulated in the idea of **conservation laws**.\n",
    "You may read more [here](https://cds.cern.ch/record/2759491/files/Conservation%20Laws%20-%20ATLAS%20Physics%20Cheat%20Sheet.pdf).\n",
    "\n",
    "Let's try to perform this two-step analysis for one event in `data_A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cut for lepton type? [False]\n",
      "Cut for lepton charge? [True]\n",
      "[[4.91e+04, 4.35e+04, 4.3e+04, 2.74e+04]] [[-1.66, -1.01, -0.0205, 0.0453]]\n",
      "[9.71e+04]\n",
      "Invariant mass: [197]\n"
     ]
    }
   ],
   "source": [
    "# This selects the first entry of the tree\n",
    "entry = tree.arrays(library=\"ak\")[:1] \n",
    "\n",
    "# Cut lepton type (electron type is 11,  muon type is 13)\n",
    "lep_type = entry['lep_type']\n",
    "sum_lep_type = lep_type[:, 0] + lep_type[:, 1] + lep_type[:, 2] + lep_type[:, 3]\n",
    "lep_type_cut_bool = (sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52)\n",
    "print(f\"Cut for lepton type? {lep_type_cut_bool}\") # True means we should remove this entry (lepton type does not match)\n",
    "\n",
    "# Cut lepton charge\n",
    "# first lepton in each event is [:, 0], 2nd lepton is [:, 1] etc\n",
    "lep_charge = entry['lep_charge']\n",
    "sum_lep_charge = lep_charge[:, 0] + lep_charge[:, 1] + lep_charge[:, 2] + lep_charge[:, 3] != 0\n",
    "print(f\"Cut for lepton charge? {sum_lep_charge}\") # True means we should remove this entry (sum of lepton charges is not equal to 0)\n",
    "\n",
    "# Calculate invariant mass of the 4-lepton state\n",
    "# [:, i] selects the i-th lepton in each event\n",
    "p4 = vector.zip({\"pt\": entry['lep_pt'], \"eta\": entry['lep_eta'], \"phi\": entry['lep_phi'], \"E\": entry['lep_E']})\n",
    "invariant_mass = (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).M * MeV # .M calculates the invariant mass\n",
    "print(entry['lep_pt'], entry['lep_eta'])\n",
    "print((p4[:,0] + p4[:, 1]).M)\n",
    "print(f\"Invariant mass: {invariant_mass}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our analysis, this entry should be removed because the lepton type does not match our requirements.\n",
    "We can turn these checks and calculations into a set of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut lepton type (electron type is 11,  muon type is 13)\n",
    "def cut_lep_type(lep_type):\n",
    "    sum_lep_type = lep_type[:, 0] + lep_type[:, 1] + lep_type[:, 2] + lep_type[:, 3]\n",
    "    lep_type_cut_bool = (sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52)\n",
    "    return lep_type_cut_bool # True means we should remove this entry (lepton type does not match)\n",
    "\n",
    "# Cut lepton charge\n",
    "def cut_lep_charge(lep_charge):\n",
    "    # first lepton in each event is [:, 0], 2nd lepton is [:, 1] etc\n",
    "    sum_lep_charge = lep_charge[:, 0] + lep_charge[:, 1] + lep_charge[:, 2] + lep_charge[:, 3] != 0\n",
    "    return sum_lep_charge # True means we should remove this entry (sum of lepton charges is not equal to 0)\n",
    "\n",
    "# Calculate invariant mass of the 4-lepton state\n",
    "# [:, i] selects the i-th lepton in each event\n",
    "def calc_mass(lep_pt, lep_eta, lep_phi, lep_E):\n",
    "    p4 = vector.zip({\"pt\": lep_pt, \"eta\": lep_eta, \"phi\": lep_phi, \"E\": lep_E})\n",
    "    invariant_mass = (p4[:, 0] + p4[:, 1] + p4[:, 2] + p4[:, 3]).M * MeV # .M calculates the invariant mass\n",
    "    return invariant_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may verify on your own that these functions give the same outputs as the previous code block.\n",
    "Now, \n",
    "    we shall apply these functions over the entire data tree using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Array [{lep_pt: [...], ...}, ..., {...}] type='4 * {lep_pt: var * float32,...'>]\n",
      "[<Array [{lep_pt: [...], ...}, ..., {...}] type='4 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='3 * {lep_pt: var * float32,...'>]\n",
      "[<Array [{lep_pt: [...], ...}, ..., {...}] type='4 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='3 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='7 * {lep_pt: var * float32,...'>]\n",
      "[<Array [{lep_pt: [...], ...}, ..., {...}] type='4 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='3 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='7 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='6 * {lep_pt: var * float32,...'>]\n",
      "[<Array [{lep_pt: [...], ...}, ..., {...}] type='4 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='3 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='7 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='6 * {lep_pt: var * float32,...'>, <Array [{lep_pt: [...], ...}, ..., {...}] type='7 * {lep_pt: var * float32,...'>]\n",
      "27 * {\n",
      "    lep_pt: var * float32,\n",
      "    lep_eta: var * float32,\n",
      "    lep_phi: var * float32,\n",
      "    lep_E: var * float32,\n",
      "    lep_charge: var * int32,\n",
      "    lep_type: var * uint32\n",
      "}\n",
      "None\n",
      "[649, 87.9, 345, 327, 107, 575, 89, ..., 272, 88.8, 247, 83.7, 129, 91.9, 122]\n",
      "27 * {\n",
      "    lep_pt: var * float32,\n",
      "    lep_eta: var * float32,\n",
      "    lep_phi: var * float32,\n",
      "    lep_E: var * float32,\n",
      "    lep_charge: var * int32,\n",
      "    lep_type: var * uint32,\n",
      "    mass: float32\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Define empty list to hold all data for this sample\n",
    "sample_data = []\n",
    "\n",
    "chunks = 5\n",
    "mass = []\n",
    "data2 = []\n",
    "\n",
    "# Perform the cuts for each data entry in the tree\n",
    "for data in tree.iterate(variables, library=\"ak\", step_size = 1000000): # the data will be in the form of an awkward array\n",
    "    # We can use data[~boolean] to remove entries from the data set\n",
    "    chunk_size = len(data['lep_type'])//chunks\n",
    "    lep_type = data['lep_type']\n",
    "    lep_charge = data['lep_charge']\n",
    "        \n",
    "    for i in range(chunks):\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i != chunks - 1:\n",
    "            data2.append(data[start:end][~((cut_lep_type(lep_type[start:end]) | cut_lep_charge(lep_charge[start:end])))])\n",
    "            #data2.append(data[start:end][~cut_lep_charge(lep_charge[start:end])])\n",
    "        else:\n",
    "            data2.append(data[start:][~((cut_lep_type(lep_type[start:]) | cut_lep_charge(lep_charge[start:])))])\n",
    "            #data2.append(data[start:][~cut_lep_charge(lep_charge[start:])])\n",
    "        print(data2)\n",
    "    from HZZ_task import cut_data, calculate_mass\n",
    "\n",
    "    data2 = []\n",
    "    #print([cut_data(i, data, data2, chunks, chunk_size, lep_type, lep_charge)for i in range(chunks)])\n",
    "    data2 = [cut_data(i, data, chunks, chunk_size, lep_type, lep_charge)for i in range(chunks)]\n",
    "    data2 = sum(data2, [])\n",
    "    data = ak.concatenate(data2)\n",
    "    print(data.type.show())\n",
    "\n",
    "    '''\n",
    "    lep_type = data['lep_type']\n",
    "    data = data[~cut_lep_type(lep_type)]\n",
    "    lep_charge = data['lep_charge']\n",
    "    data = data[~cut_lep_charge(lep_charge)]\n",
    "    #print(data.type.show())\n",
    "    '''\n",
    "    chunk_size = len(data)//chunks\n",
    "    '''\n",
    "    for i in range(chunks):\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i != chunks - 1:\n",
    "            mass.append(calc_mass(data['lep_pt'][start:end], data['lep_eta'][start:end], data['lep_phi'][start:end], data['lep_E'][start:end]))\n",
    "        else:\n",
    "            mass.append(calc_mass(data['lep_pt'][start:], data['lep_eta'][start:], data['lep_phi'][start:], data['lep_E'][start:]))\n",
    "    data['mass'] = np.concatenate(mass)'''\n",
    "\n",
    "    mass = [calculate_mass(i, data, chunks, chunk_size) for i in range(chunks)]\n",
    "    mass = np.concatenate(sum(mass, []))\n",
    "    print(mass)\n",
    "    data['mass'] = mass\n",
    "    # Append data to the whole sample data list\n",
    "    sample_data.append(data)\n",
    "#print(data['lep_pt'].fields)\n",
    "# turns sample_data back into an awkward array\n",
    "data_A = ak.concatenate(sample_data)\n",
    "data_A.type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the data using Matplotlib. \n",
    "The data will be turned into a histogram,\n",
    "    with bins of width $5 \\,\\text{GeV}$.\n",
    "Note that much of the code written here is meant for the aesthetics of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis range of the plot\n",
    "xmin = 80 * GeV\n",
    "xmax = 250 * GeV\n",
    "\n",
    "# Histogram bin setup\n",
    "step_size = 5 * GeV\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "# Creating histogram from data\n",
    "data_x,_ = np.histogram(ak.to_numpy(data_A['mass']), \n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ); # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great,\n",
    "    we managed to plot `data_A`! \n",
    "Now, \n",
    "    we have not discussed how to deal with the Monte-Carlo simulation data,\n",
    "    or even what they are for. \n",
    "Let us explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Reading Monte-Carlo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the Standard Model, \n",
    "    we can do a set of randomised simulations to produce a set of theoretical data points to compare to our ATLAS data.\n",
    "These are known as Monte-Carlo(MC) simulations.\n",
    "There is one important change to be made to the MC data before we can compare them with our ATLAS data:\n",
    " - **Weights** - The MC data was computed in ideal circumstances. \n",
    "    The real ATLAS detector has some inefficiencies,\n",
    "        which we can account for by attributing the appropriate weight to each data point.\n",
    "    The weight of a data point affects how it contributes to the histogram count for its bin.\n",
    "\n",
    "Let's open an MC file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'list': ['Zee', 'Zmumu', 'ttbar_lep'], 'color': '#6b59d3'}\n"
     ]
    }
   ],
   "source": [
    "# We open an MC data file with sample value \"Zee\" using samples and infofile for reference of filenames\n",
    "print(samples[\"Background $Z,t\\\\bar{t}$\"])\n",
    "value = samples[\"Background $Z,t\\\\bar{t}$\"][\"list\"][0]\n",
    "\n",
    "# This is now appended to our file path to retrieve the root file\n",
    "background_Zee_path = path + \"MC/mc_\"+str(infofile.infos[value][\"DSID\"])+\".\"+value+\".4lep.root\"\n",
    "\n",
    "# Accessing the file from the online directory\n",
    "tree = uproot.open(background_Zee_path + \":mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, \n",
    "    not all weights are important to our analysis. \n",
    "In our case, \n",
    "    these are:\n",
    "- `mcWeight` - specific Monte-Carlo weight associated with each event\n",
    "- `scaleFactor_PILEUP` - scale factor for pileup reweighting\n",
    "- `scaleFactor_ELE` - scale factor for electron efficiency\n",
    "- `scaleFactor_MUON`- scale factor for muon efficiency\n",
    "- `scaleFactor_LepTRIGGER` - scale factor for lepton triggers\n",
    "\n",
    "Scale factors are generally related to estimates of the efficiencies and resolutions of detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_variables = [\"mcWeight\", \"scaleFactor_PILEUP\", \"scaleFactor_ELE\", \"scaleFactor_MUON\", \"scaleFactor_LepTRIGGER\"]\n",
    "\n",
    "# For example, see below for the weights corresponding to muon rejection\n",
    "#tree[\"scaleFactor_MUON\"].arrays(library = \"ak\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally,\n",
    "    there is a cross-section weight $w_\\sigma$ associated with each MC file.\n",
    "We define this variable `xsec_weight` below. \n",
    "This weight is meant to normalise the entire Monte-Carlo distribution based on the number of events in the data.\n",
    "This is its definition:\n",
    "$$ w_\\sigma = \\frac{\\int L \\text{d}t ~ \\sigma }{\\eta \\sum_i w_i } $$\n",
    "where $\\int L \\text{d}t$ is the integrated luminosity (`lumi`),\n",
    "    $\\sigma$ is the cross section (`info[\"xsec\"]`),\n",
    "    $\\eta$ is the filter efficiency of the MC generator,\n",
    "    and $\\sum_i w_i$ gives the sum of all weights (`info[\"sumw\"]`).\n",
    "When the integrated luminosity is multiplied by the cross section,\n",
    "    it gives a measure of the total number of events during a period of data taking.\n",
    "For `data_A`,\n",
    "    the integrated luminosity has a value of $0.5 \\,\\text{fb}^{-1}$.\n",
    "\n",
    "For more on cross sections and luminosities, \n",
    "    [see this cheatsheet](https://cds.cern.ch/record/2800578/files/Cross%20Section%20and%20Luminosity%20Physics%20Cheat%20Sheet.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumi = 0.5 # fb-1 # data_A only\n",
    "# lumi = 1.9 # fb-1 # data_B only\n",
    "# lumi = 2.9 # fb-1 # data_C only\n",
    "# lumi = 4.7 # fb-1 # data_D only\n",
    "# lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "\n",
    "info = infofile.infos[value] # open infofile\n",
    "xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"red_eff\"]*info[\"sumw\"]) #*1000 to go from fb-1 to pb-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,\n",
    "    with all the weights we've defined, \n",
    "    we will calculate a total weight for an event,\n",
    "    which is the collective product of all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    runNumber: int32,\n",
      "    eventNumber: int32,\n",
      "    channelNumber: int32,\n",
      "    mcWeight: float32,\n",
      "    scaleFactor_PILEUP: float32,\n",
      "    scaleFactor_ELE: float32,\n",
      "    scaleFactor_MUON: float32,\n",
      "    scaleFactor_PHOTON: float32,\n",
      "    scaleFactor_TAU: float32,\n",
      "    scaleFactor_BTAG: float32,\n",
      "    scaleFactor_LepTRIGGER: float32,\n",
      "    scaleFactor_PhotonTRIGGER: float32,\n",
      "    trigE: bool,\n",
      "    trigM: bool,\n",
      "    trigP: bool,\n",
      "    lep_n: uint32,\n",
      "    lep_truthMatched: var * bool,\n",
      "    lep_trigMatched: var * bool,\n",
      "    lep_pt: var * float32,\n",
      "    lep_eta: var * float32,\n",
      "    lep_phi: var * float32,\n",
      "    lep_E: var * float32,\n",
      "    lep_z0: var * float32,\n",
      "    lep_charge: var * int32,\n",
      "    lep_type: var * uint32,\n",
      "    lep_isTightID: var * bool,\n",
      "    lep_ptcone30: var * float32,\n",
      "    lep_etcone20: var * float32,\n",
      "    lep_trackd0pvunbiased: var * float32,\n",
      "    lep_tracksigd0pvunbiased: var * float32,\n",
      "    met_et: float32,\n",
      "    met_phi: float32,\n",
      "    jet_n: uint32,\n",
      "    jet_pt: var * float32,\n",
      "    jet_eta: var * float32,\n",
      "    jet_phi: var * float32,\n",
      "    jet_E: var * float32,\n",
      "    jet_jvt: var * float32,\n",
      "    jet_trueflav: var * int32,\n",
      "    jet_truthMatched: var * bool,\n",
      "    jet_MV2c10: var * float32,\n",
      "    photon_n: uint32,\n",
      "    photon_truthMatched: var * bool,\n",
      "    photon_trigMatched: var * bool,\n",
      "    photon_pt: var * float32,\n",
      "    photon_eta: var * float32,\n",
      "    photon_phi: var * float32,\n",
      "    photon_E: var * float32,\n",
      "    photon_isTightID: var * bool,\n",
      "    photon_ptcone30: var * float32,\n",
      "    photon_etcone20: var * float32,\n",
      "    photon_convType: var * int32,\n",
      "    tau_n: uint32,\n",
      "    tau_pt: var * float32,\n",
      "    tau_eta: var * float32,\n",
      "    tau_phi: var * float32,\n",
      "    tau_E: var * float32,\n",
      "    tau_isTightID: var * bool,\n",
      "    tau_truthMatched: var * bool,\n",
      "    tau_trigMatched: var * bool,\n",
      "    tau_nTracks: var * int32,\n",
      "    tau_BDTid: var * float32,\n",
      "    ditau_m: float32,\n",
      "    lep_pt_syst: var * float32,\n",
      "    met_et_syst: float32,\n",
      "    jet_pt_syst: var * float32,\n",
      "    photon_pt_syst: var * float32,\n",
      "    tau_pt_syst: var * float32,\n",
      "    XSection: float32,\n",
      "    SumWeights: float32,\n",
      "    largeRjet_n: uint32,\n",
      "    largeRjet_pt: var * float32,\n",
      "    largeRjet_eta: var * float32,\n",
      "    largeRjet_phi: var * float32,\n",
      "    largeRjet_E: var * float32,\n",
      "    largeRjet_m: var * float32,\n",
      "    largeRjet_truthMatched: var * float32,\n",
      "    largeRjet_D2: var * float32,\n",
      "    largeRjet_tau32: var * float32,\n",
      "    largeRjet_pt_syst: var * float32,\n",
      "    tau_charge: var * int32\n",
      "}\n",
      "None\n",
      "mcWeight\n",
      "1941.64\n",
      "scaleFactor_PILEUP\n",
      "1.5546491\n",
      "scaleFactor_ELE\n",
      "0.93696034\n",
      "scaleFactor_MUON\n",
      "0.98378295\n",
      "scaleFactor_LepTRIGGER\n",
      "0.98683\n",
      "total_weight = 0.01781937\n"
     ]
    }
   ],
   "source": [
    "# Let's use the first event of our tree\n",
    "event = tree.arrays()[0]\n",
    "print(event.type.show())\n",
    "\n",
    "# Multiply all the important weights together\n",
    "total_weight = xsec_weight \n",
    "for variable in weight_variables:\n",
    "    total_weight = total_weight * event[variable]\n",
    "    print(variable)\n",
    "    print(event[variable])\n",
    "print(f\"{total_weight = :.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation means that in our final histogram, \n",
    "    this event will be represented with ~0.0178 of a single count in the bin.\n",
    "We can encapsulate these calculations in a single function `calc_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017819365\n"
     ]
    }
   ],
   "source": [
    "def calc_weight(weight_variables, sample, events):\n",
    "    info = infofile.infos[sample]\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    total_weight2 = xsec_weight\n",
    "    total_weight = xsec_weight*events[weight_variables[0]] * events[weight_variables[1]] * events[weight_variables[2]] * events[weight_variables[3]] * events[weight_variables[4]]\n",
    "    for variable in weight_variables:\n",
    "        total_weight2 = total_weight2 * events[variable]\n",
    "    return total_weight\n",
    "\n",
    "# Verify that we get the same answer\n",
    "print(calc_weight(weight_variables, value, event))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the cuts as before to plot the MC data.\n",
    "The code is the same as before,\n",
    "    but we make sure to add in `weight_variables` to our `tree.iterate()`,\n",
    "    and we store the weights in each event using a new dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "[None, None, None, None, None]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"NoneType\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m weight \u001b[38;5;241m=\u001b[39m [calculate_weight(i, data, chunks, chunk_size, value) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(chunks)]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(weight)\n\u001b[0;32m---> 28\u001b[0m weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#data['totalWeight'] = calc_weight(weight_variables, value, data)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotalWeight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(weight)\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"NoneType\") to list"
     ]
    }
   ],
   "source": [
    "sample_data = []\n",
    "\n",
    "# Perform the cuts for each data entry in the tree\n",
    "for data in tree.iterate(variables + weight_variables, library=\"ak\", step_size = 1000000):\n",
    "    # Cuts\n",
    "    lep_type = data['lep_type']\n",
    "    data = data[~cut_lep_type(lep_type)]\n",
    "    lep_charge = data['lep_charge']\n",
    "    data = data[~cut_lep_charge(lep_charge)]\n",
    "    \n",
    "    # Invariant Mass\n",
    "    data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_E'])\n",
    "    from HZZ_task import calculate_weight\n",
    "    # Store Monte Carlo weights in the data\n",
    "    chunk_size = len(data)//chunks\n",
    "    print(chunk_size)\n",
    "    '''for i in range(chunks):\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        for variable in weight_variables:\n",
    "            if i != chunks - 1:\n",
    "                split_data[variable] = data[variable][start:end]\n",
    "            else:\n",
    "                split_data[variable] = data[variable][start:]\n",
    "        weight.append(calc_weight(weight_variables, value, split_data))'''\n",
    "    weight = [calculate_weight(i, data, chunks, chunk_size, value) for i in range(chunks)]\n",
    "    print(weight)\n",
    "    weight = sum(weight, [])\n",
    "    #data['totalWeight'] = calc_weight(weight_variables, value, data)\n",
    "    data['totalWeight'] = np.concatenate(weight)\n",
    "    # Append data to the whole sample data list\n",
    "    sample_data.append(data)\n",
    "# turns sample_data back into an awkward array\n",
    "Background_Zee = ak.concatenate(sample_data)\n",
    "print(len(data['mass']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_x = ak.to_numpy(Background_Zee[\"mass\"]) # define list to hold the Monte Carlo histogram entries\n",
    "mc_weights = ak.to_numpy(Background_Zee[\"totalWeight\"]) # define list to hold the Monte Carlo weights\n",
    "mc_colors = samples[\"Background $Z,t\\\\bar{t}$\"]['color'] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = \"Background $Z \\\\to ee$\" # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# plot the Monte Carlo bars\n",
    "mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=True, \n",
    "                            color=mc_colors, label=mc_labels )\n",
    "\n",
    "mc_x_tot = mc_heights[0] # stacked background MC y-axis value\n",
    "\n",
    "# calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "# plot the statistical uncertainty\n",
    "main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ); # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand all the steps of our analysis,\n",
    "    all that's left is to import the entire ATLAS data and implement it.\n",
    "The `samples` dictionary and `infofile.py` will be useful for this.\n",
    "\n",
    "We will loop over all values in the `samples` dictionary.\n",
    "Depending on whether it is a data sample or MC sample, \n",
    "    `fileString` will change,\n",
    "    which opens the correct file on the open data folder.\n",
    "As before, \n",
    "    the cuts, \n",
    "    mass calculations and MC weight calculations will be performed for each sample value,\n",
    "    and then stored in the array.\n",
    "The data will all be concatenated into `all_data` for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set luminosity to 10 fb-1 for all data\n",
    "lumi = 10\n",
    "\n",
    "# Controls the fraction of all events analysed\n",
    "fraction = 1.0 # reduce this is if you want quicker runtime (implemented in the loop over the tree)\n",
    "\n",
    "# Define empty dictionary to hold awkward arrays\n",
    "all_data = {} \n",
    "\n",
    "# Loop over samples\n",
    "for s in samples: \n",
    "    # Print which sample is being processed\n",
    "    print('Processing '+s+' samples') \n",
    "\n",
    "    # Define empty list to hold data\n",
    "    frames = [] \n",
    "\n",
    "    # Loop over each file\n",
    "    for val in samples[s]['list']: \n",
    "        if s == 'data': \n",
    "            prefix = \"Data/\" # Data prefix\n",
    "        else: # MC prefix\n",
    "            prefix = \"MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "        fileString = path+prefix+val+\".4lep.root\" # file name to open\n",
    "\n",
    "\n",
    "        # start the clock\n",
    "        start = time.time() \n",
    "        print(\"\\t\"+val+\":\") \n",
    "\n",
    "        # Open file\n",
    "        with uproot.open(fileString + \":mini\") as t:\n",
    "            tree = t\n",
    "        \n",
    "        sample_data = []\n",
    "\n",
    "        # Loop over data in the tree\n",
    "        for data in tree.iterate(variables + weight_variables, \n",
    "                                 library=\"ak\", \n",
    "                                 entry_stop=tree.num_entries*fraction, # process up to numevents*fraction\n",
    "                                 step_size = 1000000): \n",
    "            # Number of events in this batch\n",
    "            nIn = len(data) \n",
    "                                 \n",
    "            # Record transverse momenta (see bonus activity for explanation)\n",
    "            data['leading_lep_pt'] = data['lep_pt'][:,0]\n",
    "            data['sub_leading_lep_pt'] = data['lep_pt'][:,1]\n",
    "            data['third_leading_lep_pt'] = data['lep_pt'][:,2]\n",
    "            data['last_lep_pt'] = data['lep_pt'][:,3]\n",
    "\n",
    "            # Cuts\n",
    "            lep_type = data['lep_type']\n",
    "            data = data[~cut_lep_type(lep_type)]\n",
    "            lep_charge = data['lep_charge']\n",
    "            data = data[~cut_lep_charge(lep_charge)]\n",
    "            \n",
    "            # Invariant Mass\n",
    "            data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_E'])\n",
    "\n",
    "            # Store Monte Carlo weights in the data\n",
    "            if 'data' not in val: # Only calculates weights if the data is MC\n",
    "                data['totalWeight'] = calc_weight(weight_variables, val, data)\n",
    "                nOut = sum(data['totalWeight']) # sum of weights passing cuts in this batch \n",
    "            else:\n",
    "                nOut = len(data)\n",
    "            elapsed = time.time() - start # time taken to process\n",
    "            print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "\n",
    "            # Append data to the whole sample data list\n",
    "            sample_data.append(data)\n",
    "\n",
    "        frames.append(ak.concatenate(sample_data)) \n",
    "\n",
    "    all_data[s] = ak.concatenate(frames) # dictionary entry is concatenated awkward arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x,_ = np.histogram(ak.to_numpy(all_data['data']['mass']), \n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "signal_x = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)']['mass']) # histogram the signal\n",
    "signal_weights = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)'].totalWeight) # get the weights of the signal events\n",
    "signal_color = samples[r'Signal ($m_H$ = 125 GeV)']['color'] # get the colour for the signal bar\n",
    "\n",
    "mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "for s in samples: # loop over samples\n",
    "    if s not in ['data', r'Signal ($m_H$ = 125 GeV)']: # if not data nor signal\n",
    "        mc_x.append( ak.to_numpy(all_data[s]['mass']) ) # append to the list of Monte Carlo histogram entries\n",
    "        mc_weights.append( ak.to_numpy(all_data[s].totalWeight) ) # append to the list of Monte Carlo weights\n",
    "        mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "        mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# plot the Monte Carlo bars\n",
    "mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=True, \n",
    "                            color=mc_colors, label=mc_labels )\n",
    "\n",
    "mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "\n",
    "# calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "# plot the signal bar\n",
    "signal_heights = main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot, \n",
    "                weights=signal_weights, color=signal_color,\n",
    "                label=r'Signal ($m_H$ = 125 GeV)')\n",
    "\n",
    "# plot the statistical uncertainty\n",
    "main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# Add text 'ATLAS Open Data' on plot\n",
    "plt.text(0.05, # x\n",
    "            0.93, # y\n",
    "            'ATLAS Open Data', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            fontsize=13 ) \n",
    "\n",
    "# Add text 'for education' on plot\n",
    "plt.text(0.05, # x\n",
    "            0.88, # y\n",
    "            'for education', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            style='italic',\n",
    "            fontsize=8 ) \n",
    "\n",
    "# Add energy and luminosity\n",
    "lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "plt.text(0.05, # x\n",
    "            0.82, # y\n",
    "            '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "            transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "# Add a label for the analysis carried out\n",
    "plt.text(0.05, # x\n",
    "            0.76, # y\n",
    "            r'$H \\rightarrow ZZ^* \\rightarrow 4\\ell$', # text \n",
    "            transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ) # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. \n",
    "We have a nice peak in the invariant mass spectrum somewhere around $125 \\, \\text{GeV}$,\n",
    "    the signature of the Higgs boson!\n",
    "\n",
    "In addition to the Higgs peak, \n",
    "    we can see another peak around $91 \\, \\text{GeV}$, \n",
    "    corresponding to the Z boson from our background.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some analysis to study how significant the signal is compared to the background. \n",
    "One method is to check a quantity known as the signal significance $S$,\n",
    "    which is defined by \n",
    "$$ S = \\frac{N_\\text{sig}}{\\sqrt{N_\\text{bg}}}  $$\n",
    "where $ N_\\text{sig} $ and $N_\\text{bg}$ are the number of signal and background points respectively.\n",
    "A larger $S$ represents a better signal-to-background ratio,\n",
    "    and a more significant signal peak.\n",
    "To calculate $N_\\text{sig}$, \n",
    "    we can look at the plot and sum over the number of events of our Monte-Carlo signal.\n",
    "The signal range roughly corresponds to the bins from $115 \\,\\text{GeV}$ to $130 \\, \\text{GeV}$.\n",
    "$N_\\text{bg}$ then corresponds to the number of background events in those same bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, \n",
    "    we must note that there are various systematic and experimental uncertainties in the operation of the experiment, \n",
    "    which we have not accounted for. \n",
    "Since our analysis is not completely accounting for these errors,\n",
    "    we can add approximate these errors by adding in a factor of $0.3N_\\text{bg}^2$ to the square root in the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal stacked height\n",
    "signal_tot = signal_heights[0] + mc_x_tot\n",
    "\n",
    "# Peak of signal\n",
    "print(signal_tot[8])\n",
    "\n",
    "# Neighbouring bins\n",
    "print(signal_tot[7:10])\n",
    "\n",
    "# Signal and background events\n",
    "N_sig = signal_tot[7:10].sum()\n",
    "N_bg = mc_x_tot[7:10].sum()\n",
    "\n",
    "# Signal significance calculation\n",
    "signal_significance = N_sig/np.sqrt(N_bg + 0.3 * N_bg**2) \n",
    "print(f\"\\nResults:\\n{N_sig = :.3f}\\n{N_bg = :.3f}\\n{signal_significance = :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go - we have a significance of $2.208$.\n",
    "Our work is done here... or so you may think!\n",
    "Here are some additional things you can do to play with this notebook:\n",
    "\n",
    "* Check how many events are being thrown away by each cut \n",
    "* Add more cuts from the [Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#se0040) to see how it affects the signal-to-background ratio\n",
    "* Add a plot to show the ratio between data and MC \n",
    "* Add a plot to show the invariant mass distribution of the sub-leading lepton pair, like [Figure 1 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#fg0010)\n",
    "* Get the estimated numbers of events, like [Table 3 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#tl0030)\n",
    "* Add a plot of m12 against m34, like [Figure 3 of the Higgs discovery paper](https://www.sciencedirect.com/science/article/pii/S037026931200857X#fg0030)\n",
    "* Pretty much anything you like, let your imagination run wild!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus activity: Transverse momentum cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better than a significance of $2.208$?\n",
    "Well, \n",
    "    there are many other kinematic variables in our data that can help us discriminate between signal and background.\n",
    "One possible variable to look at is the transverse momentum $p_t$, \n",
    "stored as `lep_pt` in the tree.\n",
    "In our final analysis code block,\n",
    "    we recorded the transverse momenta of the leading, sub-leading, third-leading and last leptons \n",
    "    (the data had already been arranged in descending order of $p_t$).\n",
    "\n",
    "To get a feel of the lepton momenta, \n",
    "    let's plot the various MC signal and backgrounds to see if we can make any cuts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis range of the plot\n",
    "xmin = 0 \n",
    "xmax = 200 \n",
    "\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "                    \n",
    "pt_keys = ['leading_lep_pt', 'sub_leading_lep_pt', 'third_leading_lep_pt', 'last_lep_pt']\n",
    "signal_all = []\n",
    "for key in pt_keys:\n",
    "    signal_all.append(ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)'][key]*MeV)) \n",
    "\n",
    "signal_weights = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)'].totalWeight) # get the weights of the signal events\n",
    "signal_color = samples[r'Signal ($m_H$ = 125 GeV)']['color'] # get the colour for the signal bar\n",
    "\n",
    "mc_all = [] # define lists to hold the Monte Carlo histogram entries\n",
    "mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "for index in range(4):\n",
    "    mc_all.append([]) \n",
    "    for s in samples: # loop over samples\n",
    "        if s not in ['data', r'Signal ($m_H$ = 125 GeV)']: # if not data nor signal\n",
    "            mc_all[index].append(ak.to_numpy(all_data[s][pt_keys[index]]*MeV)) \n",
    "\n",
    "for s in samples: # loop over samples\n",
    "    if s not in ['data', r'Signal ($m_H$ = 125 GeV)']: # if not data nor signal\n",
    "        mc_weights.append( ak.to_numpy(all_data[s].totalWeight) ) # append to the list of Monte Carlo weights\n",
    "        mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "        mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "fig, ax = plt.subplots(4,1, figsize = (6,12))\n",
    "\n",
    "for axis in range(4):\n",
    "    # plot the Monte Carlo bars\n",
    "    ax[axis].hist(mc_all[axis], bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=False, \n",
    "                            color=mc_colors, label=mc_labels, histtype=u'step')\n",
    "\n",
    "    # plot the signal bar\n",
    "    ax[axis].hist(signal_all[axis], bins=bin_edges,\n",
    "                    weights=signal_weights, color=signal_color,\n",
    "                    label=r'Signal ($m_H$ = 125 GeV)', histtype=u'step');\n",
    "\n",
    "    # set the x-limit of the main axes\n",
    "    ax[axis].set_xlim(left=xmin, right=xmax) \n",
    "\n",
    "    # separation of x axis minor ticks\n",
    "    ax[axis].xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "    # set the axis tick parameters for the main axes\n",
    "    ax[axis].tick_params(which='both', # ticks on both x and y axes\n",
    "                            direction='in', # Put ticks inside and outside the axes\n",
    "                            top=True, # draw ticks on the top axis\n",
    "                            right=True ) # draw ticks on right axis\n",
    "\n",
    "    # write y-axis label for main axes\n",
    "    ax[axis].set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                            y=1, horizontalalignment='right') \n",
    "\n",
    "    # set y-axis limits for main axes\n",
    "    ax[axis].set_ylim( bottom=0, top=100 )\n",
    "\n",
    "    # add minor ticks on y-axis for main axes\n",
    "    ax[axis].yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "\n",
    "# x-axis label\n",
    "labels = [r'Leading lepton $p_t$ [GeV]', r'Sub-leading lepton $p_t$ [GeV]',\n",
    "          r'Third-leading lepton $p_t$ [GeV]',r'Last lepton $p_t$ [GeV]']\n",
    "for axis in range(4):\n",
    "    ax[axis].set_xlabel(labels[axis],\n",
    "                        fontsize=10, x=1, horizontalalignment='right' )\n",
    "# draw the legend\n",
    "ax[0].legend( frameon=False ) # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from these plots that we could improve the signal-to-background by cutting away regions that have large backgrounds but small signals. Let us make the following selections:\n",
    "- $>30\\, \\text{GeV}$ for leading $p_t$\n",
    "- $>20\\, \\text{GeV}$ for sub-leading $p_t$\n",
    "- $>10\\, \\text{GeV}$ for third-leading $p_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoffs = [30, 20, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why don't we add an upper bound cut? \n",
    "This is because the high energy tails of our distribution are often important parts of the data.\n",
    "For one, \n",
    "    if we cut away the high energy parts of the data,\n",
    "    we could potentially be removing a portion that reveals new physics.\n",
    "Such new physics could lie beyond the standard model,\n",
    "    only appearing at higher energies!\n",
    "Thus, \n",
    "    we should be careful not to purposefully exclude such data from our analysis.\n",
    "\n",
    "Now that we have our cutoffs,\n",
    "    we should implement them in our analysis.\n",
    "Note the addition of the lines in the form\n",
    "\n",
    "`data = data[data['leading_lep_pt'] * MeV > cutoffs[0]]`\n",
    "\n",
    "which keeps only events that lie above the cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set luminosity to 10 fb-1 for all data\n",
    "lumi = 10\n",
    "\n",
    "# Controls the fraction of all events analysed\n",
    "fraction = 1.0 # reduce this is if you want quicker runtime (implemented in the loop over the tree)\n",
    "\n",
    "# Define empty dictionary to hold awkward arrays\n",
    "all_data = {} \n",
    "\n",
    "# Loop over samples\n",
    "for s in samples: \n",
    "    # Print which sample is being processed\n",
    "    print('Processing '+s+' samples') \n",
    "\n",
    "    # Define empty list to hold data\n",
    "    frames = [] \n",
    "\n",
    "    # Loop over each file\n",
    "    for val in samples[s]['list']: \n",
    "        if s == 'data': \n",
    "            prefix = \"Data/\" # Data prefix\n",
    "        else: # MC prefix\n",
    "            prefix = \"MC/mc_\"+str(infofile.infos[val][\"DSID\"])+\".\"\n",
    "        fileString = path+prefix+val+\".4lep.root\" # file name to open\n",
    "\n",
    "\n",
    "        # start the clock\n",
    "        start = time.time() \n",
    "        print(\"\\t\"+val+\":\") \n",
    "\n",
    "        # Open file\n",
    "        with uproot.open(fileString + \":mini\") as t:\n",
    "            tree = t\n",
    "        \n",
    "        sample_data = []\n",
    "\n",
    "        # Loop over data in the tree\n",
    "        for data in tree.iterate(variables + weight_variables, \n",
    "                                 library=\"ak\", \n",
    "                                 entry_stop=tree.num_entries*fraction, # process up to numevents*fraction\n",
    "                                 step_size = 1000000): \n",
    "            # Number of events in this batch\n",
    "            nIn = len(data) \n",
    "                                 \n",
    "            # Transverse momentum records and cuts\n",
    "            data['leading_lep_pt'] = data['lep_pt'][:,0]\n",
    "            data['sub_leading_lep_pt'] = data['lep_pt'][:,1]\n",
    "            data['third_leading_lep_pt'] = data['lep_pt'][:,2]\n",
    "            data['last_lep_pt'] = data['lep_pt'][:,3]\n",
    "\n",
    "            data = data[data['leading_lep_pt'] * MeV > cutoffs[0]]\n",
    "            data = data[data['sub_leading_lep_pt'] * MeV > cutoffs[1]]\n",
    "            data = data[data['third_leading_lep_pt'] * MeV > cutoffs[2]]\n",
    "\n",
    "            # Cuts\n",
    "            lep_type = data['lep_type']\n",
    "            data = data[~cut_lep_type(lep_type)]\n",
    "            lep_charge = data['lep_charge']\n",
    "            data = data[~cut_lep_charge(lep_charge)]\n",
    "            \n",
    "            # Invariant Mass\n",
    "            data['mass'] = calc_mass(data['lep_pt'], data['lep_eta'], data['lep_phi'], data['lep_E'])\n",
    "\n",
    "            # Store Monte Carlo weights in the data\n",
    "            if 'data' not in val: # Only calculates weights if the data is MC\n",
    "                data['totalWeight'] = calc_weight(weight_variables, val, data)\n",
    "                nOut = sum(data['totalWeight']) # sum of weights passing cuts in this batch \n",
    "            else:\n",
    "                nOut = len(data)\n",
    "            elapsed = time.time() - start # time taken to process\n",
    "            print(\"\\t\\t nIn: \"+str(nIn)+\",\\t nOut: \\t\"+str(nOut)+\"\\t in \"+str(round(elapsed,1))+\"s\") # events before and after\n",
    "\n",
    "            # Append data to the whole sample data list\n",
    "            sample_data.append(data)\n",
    "\n",
    "        frames.append(ak.concatenate(sample_data)) \n",
    "\n",
    "    all_data[s] = ak.concatenate(frames) # dictionary entry is concatenated awkward arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis range of the plot\n",
    "xmin = 80 * GeV\n",
    "xmax = 250 * GeV\n",
    "\n",
    "bin_edges = np.arange(start=xmin, # The interval includes this value\n",
    "                    stop=xmax+step_size, # The interval doesn't include this value\n",
    "                    step=step_size ) # Spacing between values\n",
    "bin_centres = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "data_x,_ = np.histogram(ak.to_numpy(all_data['data']['mass']), \n",
    "                        bins=bin_edges ) # histogram the data\n",
    "data_x_errors = np.sqrt( data_x ) # statistical error on the data\n",
    "\n",
    "signal_x = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)']['mass']) # histogram the signal\n",
    "signal_weights = ak.to_numpy(all_data[r'Signal ($m_H$ = 125 GeV)'].totalWeight) # get the weights of the signal events\n",
    "signal_color = samples[r'Signal ($m_H$ = 125 GeV)']['color'] # get the colour for the signal bar\n",
    "\n",
    "mc_x = [] # define list to hold the Monte Carlo histogram entries\n",
    "mc_weights = [] # define list to hold the Monte Carlo weights\n",
    "mc_colors = [] # define list to hold the colors of the Monte Carlo bars\n",
    "mc_labels = [] # define list to hold the legend labels of the Monte Carlo bars\n",
    "\n",
    "for s in samples: # loop over samples\n",
    "    if s not in ['data', r'Signal ($m_H$ = 125 GeV)']: # if not data nor signal\n",
    "        mc_x.append( ak.to_numpy(all_data[s]['mass']) ) # append to the list of Monte Carlo histogram entries\n",
    "        mc_weights.append( ak.to_numpy(all_data[s].totalWeight) ) # append to the list of Monte Carlo weights\n",
    "        mc_colors.append( samples[s]['color'] ) # append to the list of Monte Carlo bar colors\n",
    "        mc_labels.append( s ) # append to the list of Monte Carlo legend labels\n",
    "\n",
    "# *************\n",
    "# Main plot \n",
    "# *************\n",
    "main_axes = plt.gca() # get current axes\n",
    "\n",
    "# plot the data points\n",
    "main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors,\n",
    "                    fmt='ko', # 'k' means black and 'o' is for circles \n",
    "                    label='Data') \n",
    "\n",
    "# plot the Monte Carlo bars\n",
    "mc_heights = main_axes.hist(mc_x, bins=bin_edges, \n",
    "                            weights=mc_weights, stacked=True, \n",
    "                            color=mc_colors, label=mc_labels )\n",
    "\n",
    "mc_x_tot = mc_heights[0][-1] # stacked background MC y-axis value\n",
    "\n",
    "# calculate MC statistical uncertainty: sqrt(sum w^2)\n",
    "mc_x_err = np.sqrt(np.histogram(np.hstack(mc_x), bins=bin_edges, weights=np.hstack(mc_weights)**2)[0])\n",
    "\n",
    "# plot the signal bar\n",
    "main_axes.hist(signal_x, bins=bin_edges, bottom=mc_x_tot, \n",
    "                weights=signal_weights, color=signal_color,\n",
    "                label=r'Signal ($m_H$ = 125 GeV)')\n",
    "\n",
    "# plot the statistical uncertainty\n",
    "main_axes.bar(bin_centres, # x\n",
    "                2*mc_x_err, # heights\n",
    "                alpha=0.5, # half transparency\n",
    "                bottom=mc_x_tot-mc_x_err, color='none', \n",
    "                hatch=\"////\", width=step_size, label='Stat. Unc.' )\n",
    "\n",
    "# set the x-limit of the main axes\n",
    "main_axes.set_xlim( left=xmin, right=xmax ) \n",
    "\n",
    "# separation of x axis minor ticks\n",
    "main_axes.xaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# set the axis tick parameters for the main axes\n",
    "main_axes.tick_params(which='both', # ticks on both x and y axes\n",
    "                        direction='in', # Put ticks inside and outside the axes\n",
    "                        top=True, # draw ticks on the top axis\n",
    "                        right=True ) # draw ticks on right axis\n",
    "\n",
    "# x-axis label\n",
    "main_axes.set_xlabel(r'4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]',\n",
    "                    fontsize=13, x=1, horizontalalignment='right' )\n",
    "\n",
    "# write y-axis label for main axes\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV',\n",
    "                        y=1, horizontalalignment='right') \n",
    "\n",
    "# set y-axis limits for main axes\n",
    "main_axes.set_ylim( bottom=0, top=np.amax(data_x)*1.6 )\n",
    "\n",
    "# add minor ticks on y-axis for main axes\n",
    "main_axes.yaxis.set_minor_locator( AutoMinorLocator() ) \n",
    "\n",
    "# Add text 'ATLAS Open Data' on plot\n",
    "plt.text(0.05, # x\n",
    "            0.93, # y\n",
    "            'ATLAS Open Data', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            fontsize=13 ) \n",
    "\n",
    "# Add text 'for education' on plot\n",
    "plt.text(0.05, # x\n",
    "            0.88, # y\n",
    "            'for education', # text\n",
    "            transform=main_axes.transAxes, # coordinate system used is that of main_axes\n",
    "            style='italic',\n",
    "            fontsize=8 ) \n",
    "\n",
    "# Add energy and luminosity\n",
    "lumi_used = str(lumi*fraction) # luminosity to write on the plot\n",
    "plt.text(0.05, # x\n",
    "            0.82, # y\n",
    "            '$\\sqrt{s}$=13 TeV,$\\int$L dt = '+lumi_used+' fb$^{-1}$', # text\n",
    "            transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "# Add a label for the analysis carried out\n",
    "plt.text(0.05, # x\n",
    "            0.76, # y\n",
    "            r'$H \\rightarrow ZZ^* \\rightarrow 4\\ell$', # text \n",
    "            transform=main_axes.transAxes ) # coordinate system used is that of main_axes\n",
    "\n",
    "# draw the legend\n",
    "main_axes.legend( frameon=False ) # no box around the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Significance (post $p_t$ cuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check our signal significance to see if it has improved after our cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal stacked height\n",
    "signal_tot = signal_heights[0] + mc_x_tot\n",
    "\n",
    "# Peak of signal\n",
    "print(signal_tot[8])\n",
    "\n",
    "# Neighbouring bins\n",
    "print(signal_tot[7:10])\n",
    "\n",
    "# Signal and background events\n",
    "N_sig = signal_tot[7:10].sum()\n",
    "N_bg = mc_x_tot[7:10].sum()\n",
    "\n",
    "# Signal significance calculation\n",
    "signal_significance = N_sig/np.sqrt(N_bg + 0.3 * N_bg**2) \n",
    "print(f\"\\nResults:\\n{N_sig = :.3f}\\n{N_bg = :.3f}\\n{signal_significance = :.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! \n",
    "With the addition of those cuts,\n",
    "    our significance increased from $2.108$ to $2.416$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequently Asked Questions (FAQ)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: How do we know the mass of the Higgs boson is $125 \\,\\text{GeV}$?\n",
    "\n",
    "**A**: The mass of the Higgs boson is not predicted by the Standard Model. \n",
    "It is a free parameter of the theory.\n",
    "To determine its value,\n",
    "    multiple analyses must be performed,\n",
    "    similar to this one,\n",
    "    but for different decay channels.\n",
    "In doing this, \n",
    "    we spot a consistent peak in mass at $125 \\,\\text{GeV}$ across the different decays,\n",
    "    which is strong evidence for it being the mass of the Higgs. \n",
    "\n",
    "**Q**: Why do we filter for more than four leptons, and not exactly four leptons?\n",
    "\n",
    "**A**: The lepton recognition algorithms are not always perfect. \n",
    "Sometimes,\n",
    "    there may be additional particles in the process which are misidentified as leptons.\n",
    "This leads to five or even six \"leptons\" in a four-lepton process.\n",
    "However, \n",
    "    since the leptons are sorted in order of their energies, \n",
    "    we always look at the four most energetic leptons,\n",
    "    which are most likely to be part of the process of interest.\n",
    "Nevertheless,\n",
    "    you are welcome to add a cut to the number of leptons. \n",
    "See if it improves the signal significance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
